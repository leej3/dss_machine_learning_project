---
title: "Activity classification using accelerometer data"
author: "John Lee"
date: "24 October 2015"
output: html_document
---

#Summary
This report describes the use of machine learning algorithms to develop a model to predict different classes of exercise from an accelerometer dataset. Accelerometers were used to detect activity on a dumbell, a belt and a forearm during weight-lifting. Improper technique was used in 4 out of 5 "classe" categories. The data was collected in 6 individuals. The final model achieved an out of sample accuracy of 32%.
For more information on the dataset see  http://groupware.les.inf.puc-rio.br/har 

##Loading libraries
```{r load libraries, warning=FALSE,results='hide'}
library(plyr, quietly= TRUE, warn.conflicts = FALSE)
library(dplyr, quietly= TRUE, warn.conflicts = FALSE)
library(caret, quietly= TRUE, warn.conflicts = FALSE)
library(splines, quietly= TRUE, warn.conflicts = FALSE)
library(parallel, quietly= TRUE, warn.conflicts = FALSE)

library(ggplot2, quietly= TRUE, warn.conflicts = FALSE)
library(data.table, quietly= TRUE, warn.conflicts = FALSE)
library(survival, quietly = TRUE,warn.conflicts = FALSE)
library(gbm, quietly= TRUE, warn.conflicts = FALSE)
library(randomForest, quietly= TRUE, warn.conflicts = FALSE)
```

## Loading and processing the data
The dataset was loaded and partitioned into training and testing sets. As the data consisted of many samples measured from 6 individuals, the test and validation set was each chosen to be one of these individuals. The training set was additionally subsampled, still using all 4 remaining individuals, to provide a smaller set to initially test algorithms on. This initial testing was performed to get a rough estimate of the accuracy and computation time for each type of model. 

```{r download and load data}
if (!file.exists("train_set.csv")){ 
train_url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
download.file(train_url, "train_set.csv", method = 'curl')
}
f_training <- fread('train_set.csv')
training <- filter(f_training,user_name != "carlitos"& user_name !="pedro")
testing <-  filter(f_training, user_name== "carlitos")
validation <- filter(f_training, user_name == "pedro")

# Sub sampling data for a quick test of different methods
in_sub_Train <-  createDataPartition(training[,classe], p = 0.1)
    sub_training <-  training[in_sub_Train$Resample1]
```

Non-numerical and irrelevant metadata variables were removed to leave only the variable for classification (classe) and predictor variables. All variables contained sufficient variance to be potentially useful as predictors. All variables were used as the final model presented could be computed within a reasonable amount of time.

```{r tidy up}
extract_useful_variables <- function(dataset){ 
nas_in_vars <- apply(is.na(dataset),2,sum)
dataset <- select(dataset, which(nas_in_vars==0))
dataset <- select(dataset,which(sapply(dataset, class)!="character" ), classe, -c(1:7))
dataset$classe <- as.factor(dataset$classe)
dataset
}
sub_training <- extract_useful_variables(sub_training)
training <- extract_useful_variables(training)
# No variables have a low variability
near_zero_variables <- any(nearZeroVar(training,saveMetrics = TRUE)$nzv)
```

## A simple decision tree
A simple decision tree constructed using the rpart package is computed rapidly but has reasonably poor accuracy as judged by the out of bag error rate, an estimate of the out of sample error rate calculated from the training set.


```{r rpart, cache= TRUE, warning = FALSE, echo=TRUE}
time_rpart <-system.time({set.seed(1000)
    mod_fit <- train(classe ~.,
                    method = "rpart",
                    preProcess = c("center","scale"),
                    data = sub_training)})
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_rpart
conf_matrix$overall[1]
```

The model produced from the decision tree algorithm predicted classe with a `r conf_matrix$overall[1]*100`% in the test dataset. This performance was not improved when principal components of the predictors were used:

```{r rpart with pca, cache= TRUE, warning = FALSE, echo=TRUE}
time_rpart_pca <- system.time({set.seed(1000)
    ctrl <- trainControl(preProcOptions = list(thresh = 0.8))
    mod_fit <- train(classe ~.,
                     method = "rpart",
                     preProcess = "pca",
                     trControl = ctrl,    
                     data = sub_training)})
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_rpart_pca
conf_matrix$overall[1]
```

## A boosting technique
```{r gbm on 10th of data, cache= TRUE, warning = FALSE, echo=TRUE}
time_gbm <- system.time({set.seed(1000)
mod_fit <- train(classe ~.,
                     method = "gbm", verbose = FALSE,
                     data = sub_training)
}
)
within_training_accuracy <- confusionMatrix(training$classe,predict(mod_fit,training))
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_gbm
conf_matrix$overall[1]
```
The generalised boosted regression modelling (gbm package) did produce a model that classified well within the training dataset (`r 100*within_training_accuracy$overall[1]`% accuracy) but it still had poor out of sample accuracy due to overfitting.

## Using a random forest
Similarly, the random forest algorithm provided good accuracy only within the training set. 

```{r rf, cache=TRUE, echo=TRUE}
time_random_forest <- system.time({set.seed(1000)
mod_fit <- train(classe ~.,
                     method = "rf",
                     data = sub_training)
}
)
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_random_forest
conf_matrix$overall[1]
```

Reducing the number of predictor variables using principal component analysis helped to improve the speed of the random forest algorithm:

```{r rf with pca,cache = TRUE, warning=FALSE}
time_random_forest_pca <- system.time({set.seed(1000)
ctrl <- trainControl(preProcOptions = list(thresh = 0.8))
mod_fit <- train(classe ~.,
                     method = "rf",
                     preProcess = "pca",
                     trControl = ctrl,    
                     data = sub_training)
}
)
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_random_forest_pca
conf_matrix$overall[1]
```
The computation of the random forest model is time-consuming in part due to the fact that the caret package optimises the number of variables sampled at each node when constructing the trees. When using the randomForest package instead the model to be built within an acceptable time. The speed with which this model is computed makes it more practical to use.

```{r random forest defining mtry and ntree, cache= TRUE, warning = FALSE, echo=TRUE}
time_rf_without_caret <- system.time({set.seed(1000);mod_fit <- randomForest(formula = classe~., data = sub_training, importance = FALSE)})
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_rf_without_caret
conf_matrix$overall[1]
```
This algorithm was then used on the full training set  to produce a model from as much data as possible.

# The final model
```{r final model, cache= TRUE, warning = FALSE, echo=TRUE}
time_fm <- system.time({set.seed(1000);final_model <- randomForest(formula = classe~., data = training, importance = FALSE)})
conf_matrix <- confusionMatrix(testing$classe,predict(mod_fit,testing))
time_fm
conf_matrix$overall[1]
```



Classification of the validation dataset provides an estimate of the out of sample error rate for the final model:

```{r out of sample error, cache = TRUE}
confusion_matrix <- confusionMatrix(validation$classe,predict(final_model,validation))
confusion_matrix
```
The model has an out of sample accuracy of 32% prinicpally due to the poor detection of the "C" and "D" activities.

```